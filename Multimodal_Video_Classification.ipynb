{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal Video Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "53b4f0aae5c345ab92bf78707f42bb39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ea92ca46805f4b2faf635f2fb710a0da",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_842f47917edd4f4baf8d4dbccd4df745",
              "IPY_MODEL_a47d4544ae954f74926522996fc315aa",
              "IPY_MODEL_2a2a9242083e46e7b224da24daf24ec1"
            ]
          }
        },
        "ea92ca46805f4b2faf635f2fb710a0da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "842f47917edd4f4baf8d4dbccd4df745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cac91f27c8cc4a60a1faacd725e16de7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ad5814048e04ae09d69e20907e19f70"
          }
        },
        "a47d4544ae954f74926522996fc315aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_600aaca6325149678e42fd9899833ce0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 24,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 24,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46199b82036d4cf6b94a14b1a418241a"
          }
        },
        "2a2a9242083e46e7b224da24daf24ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0b85b6f1fec489e95868f8d4690d602",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 24/24 [10:57&lt;00:00, 27.92s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec378a3211aa4516bd52989dca8fee53"
          }
        },
        "cac91f27c8cc4a60a1faacd725e16de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ad5814048e04ae09d69e20907e19f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "600aaca6325149678e42fd9899833ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46199b82036d4cf6b94a14b1a418241a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0b85b6f1fec489e95868f8d4690d602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec378a3211aa4516bd52989dca8fee53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8294133c63194c23b8aae22abe3faf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aab819c578ed425cbf460199474c7b94",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_231ca45bf0b740ed95e417c99d443e47",
              "IPY_MODEL_a73a5288eee9491db0f604002306f8ed",
              "IPY_MODEL_f5e1455fcb724eaeb58de862e4018321"
            ]
          }
        },
        "aab819c578ed425cbf460199474c7b94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "231ca45bf0b740ed95e417c99d443e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9355fc0ec1384d4caa8581b66e2b8541",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa372152c2f54125abc089abfc3c4d97"
          }
        },
        "a73a5288eee9491db0f604002306f8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_acfded5535ac4c7aadcaf521fc4c00ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 20,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efd5ee412605489abde5e762c03ffc3b"
          }
        },
        "f5e1455fcb724eaeb58de862e4018321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_93dbc88e1609499d991995a6d40e59f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/20 [00:02&lt;00:00,  4.91it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0d24e5b3b324ead8382451fa638960f"
          }
        },
        "9355fc0ec1384d4caa8581b66e2b8541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa372152c2f54125abc089abfc3c4d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acfded5535ac4c7aadcaf521fc4c00ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efd5ee412605489abde5e762c03ffc3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93dbc88e1609499d991995a6d40e59f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0d24e5b3b324ead8382451fa638960f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adf7723d52e24be4b806d8e6bd1bd3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9988b21a637457ba05bb1c04f572f34",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31033da9c7a54d20966833cf9d813be0",
              "IPY_MODEL_782982516b3f47cb8e1602a97ca50d28",
              "IPY_MODEL_18c12abc1a3b49b99a6d627abd99ea25"
            ]
          }
        },
        "d9988b21a637457ba05bb1c04f572f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31033da9c7a54d20966833cf9d813be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab1bc4605fe54f398eb0716d6cc908e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0aced1e3ba94017bffc49d1bc5fc6ab"
          }
        },
        "782982516b3f47cb8e1602a97ca50d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3614b68d294e40f9be82b105a27a38e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8606a161b9a64a16aae91d58b7a3c78f"
          }
        },
        "18c12abc1a3b49b99a6d627abd99ea25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_06b96664547949ee95f4435fd187945d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00, 30.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67e9a00c3d4e46e692821786727d0cad"
          }
        },
        "ab1bc4605fe54f398eb0716d6cc908e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0aced1e3ba94017bffc49d1bc5fc6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3614b68d294e40f9be82b105a27a38e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8606a161b9a64a16aae91d58b7a3c78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06b96664547949ee95f4435fd187945d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67e9a00c3d4e46e692821786727d0cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJXlv2ZQC221"
      },
      "source": [
        "### A brief about Challenge\n",
        "\n",
        "* Problem Statement\n",
        "  > Paper ref. - https://sci-hub.se/10.1145/2818346.2829994\n",
        "\n",
        "  > To assign a single emotion label to the video clip from the eight universal emotions (Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised and Neutral) on the given multimodal dataset. Use the first 20 actors for the training and rest(04) for testing.\n",
        "\n",
        "* Approach\n",
        "\n",
        "  > ConvNet (Spatial features) + LSTM (temporal features)\n",
        "  \n",
        "* Metrics\n",
        "  > Confusion Matrix + Precision + Recall + F1 score\n",
        "\n",
        "* References - \n",
        "1. The Ryerson Audio-Visual Database of Emotional Speech and Song(RAVDESS) Dataset - https://zenodo.org/record/1188976\n",
        "2. https://discuss.pytorch.org/t/how-upload-sequence-of-image-on-video-classification/24865 \n",
        "3. https://discuss.pytorch.org/t/solved-concatenate-time-distributed-cnn-with-lstm/15435\n",
        "4. https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/#heading4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBN97oSGJZ_W"
      },
      "source": [
        "#### Necessary Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbEvQL5JJYxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac5b676-114f-4f23-f10c-1f82f0b6c751"
      },
      "source": [
        "!pip install -q torchinfo\n",
        "!apt-get install tree\n",
        "\n",
        "import os\n",
        "import io\n",
        "import zipfile\n",
        "from collections import OrderedDict\n",
        "import shutil\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchinfo import summary\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from torchvision import datasets, models, transforms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (81.9 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB5oTg621Jl5"
      },
      "source": [
        "#### Setup Dataset\n",
        "\n",
        "> The format of the dataset provided to us is in the form of a zip file, we unzip and restructure into a train and test dir. After initial restructuring the dataset is stored:\n",
        "\n",
        "* from: /content/audio_video/\\\n",
        "├── Actor_01 \\\n",
        "├── Actor_02 \\\n",
        "├── Actor_03 \\\n",
        "├── Actor_04 \\\n",
        "    ...\n",
        "\n",
        "* to:/content/audio_video/train/\\\n",
        "├── angry\\\n",
        "│   ├── Actor_01\\\n",
        "│   ├── Actor_02\\\n",
        "    ...\\\n",
        "├── calm\\\n",
        "│   ├── Actor_01\\\n",
        "│   ├── Actor_02\\\n",
        "    ... \n",
        "\n",
        "Similarly for test dir ...\n",
        "\n",
        "After initial observation, we realize that the task is "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "53b4f0aae5c345ab92bf78707f42bb39",
            "ea92ca46805f4b2faf635f2fb710a0da",
            "842f47917edd4f4baf8d4dbccd4df745",
            "a47d4544ae954f74926522996fc315aa",
            "2a2a9242083e46e7b224da24daf24ec1",
            "cac91f27c8cc4a60a1faacd725e16de7",
            "4ad5814048e04ae09d69e20907e19f70",
            "600aaca6325149678e42fd9899833ce0",
            "46199b82036d4cf6b94a14b1a418241a",
            "d0b85b6f1fec489e95868f8d4690d602",
            "ec378a3211aa4516bd52989dca8fee53"
          ]
        },
        "id": "VIJMuzQBsuoe",
        "outputId": "0f316ccb-f8b0-479d-a65e-2c7a15b9caf4"
      },
      "source": [
        "def download_dataset(filename='/content/drive/MyDrive/HMI/audio-video.zip',\n",
        "                     dirname='/content/audio_video/',\n",
        "                     preprocessed=True):\n",
        "  ''' \n",
        "  Mount Google Drive and recursively unzip the dataset \n",
        "  to their respective dirs inside a dir passed as an arg\n",
        "  '''\n",
        "  \n",
        "  # mount google drive\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  \n",
        "  if preprocessed:\n",
        "    !unzip -q /content/drive/MyDrive/HMI/audio_video.zip\n",
        "    shutil.move('/content/content/audio_video', '/content/audio_video')\n",
        "    shutil.rmtree('/content/content')\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/HMI/train.csv')\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/HMI/test.csv')\n",
        "  else:\n",
        "    with zipfile.ZipFile(filename) as zip_files:\n",
        "      for zip_file in tqdm(zip_files.namelist()[1:]):\n",
        "        data = io.BytesIO(zip_files.read(zip_file))\n",
        "        _zip_file = zipfile.ZipFile(data)\n",
        "        _zip_file.extractall(dirname)\n",
        "\n",
        "download_dataset(preprocessed=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53b4f0aae5c345ab92bf78707f42bb39",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLU0J_l6U0dK"
      },
      "source": [
        "# dir structure\n",
        "!tree -d /content/audio_video/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93-5eunon99b"
      },
      "source": [
        "# defining train and test dir paths\n",
        "TRAIN_DIR = '/content/audio_video/train'\n",
        "TEST_DIR = '/content/audio_video/test'\n",
        "\n",
        "idx2class = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
        "class2idx = {cls:idx for idx, cls in enumerate(idx2class)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vD3rhmrnayR"
      },
      "source": [
        "def get_emotion(file):\n",
        "  ''' \n",
        "  returns the emotion based upon the filename\n",
        "  '''\n",
        "  if file.startswith('02-01-01'):\n",
        "    return f'neutral'\n",
        "  elif file.startswith('02-01-02'):\n",
        "    return f'calm'\n",
        "  elif file.startswith('02-01-03'):\n",
        "    return f'happy'\n",
        "  elif file.startswith('02-01-04'):\n",
        "    return f'sad'\n",
        "  elif file.startswith('02-01-05'):\n",
        "    return f'angry'\n",
        "  elif file.startswith('02-01-06'):\n",
        "    return f'fearful'\n",
        "  elif file.startswith('02-01-07'):\n",
        "    return f'disgust'\n",
        "  elif file.startswith('02-01-08'):\n",
        "    return f'surprised'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8294133c63194c23b8aae22abe3faf6f",
            "aab819c578ed425cbf460199474c7b94",
            "231ca45bf0b740ed95e417c99d443e47",
            "a73a5288eee9491db0f604002306f8ed",
            "f5e1455fcb724eaeb58de862e4018321",
            "9355fc0ec1384d4caa8581b66e2b8541",
            "aa372152c2f54125abc089abfc3c4d97",
            "acfded5535ac4c7aadcaf521fc4c00ce",
            "efd5ee412605489abde5e762c03ffc3b",
            "93dbc88e1609499d991995a6d40e59f1",
            "f0d24e5b3b324ead8382451fa638960f",
            "adf7723d52e24be4b806d8e6bd1bd3f8",
            "d9988b21a637457ba05bb1c04f572f34",
            "31033da9c7a54d20966833cf9d813be0",
            "782982516b3f47cb8e1602a97ca50d28",
            "18c12abc1a3b49b99a6d627abd99ea25",
            "ab1bc4605fe54f398eb0716d6cc908e7",
            "c0aced1e3ba94017bffc49d1bc5fc6ab",
            "3614b68d294e40f9be82b105a27a38e7",
            "8606a161b9a64a16aae91d58b7a3c78f",
            "06b96664547949ee95f4435fd187945d",
            "67e9a00c3d4e46e692821786727d0cad"
          ]
        },
        "id": "wVTP5ql2aEcF",
        "outputId": "4901b267-7d09-4ae5-aede-11259312d2c0"
      },
      "source": [
        "def structure_dir(src='/content/audio_video', n_samples=20):\n",
        "  '''\n",
        "    returns a reorganized directory structure by splitting the dataset into TRAIN/TEST and includes only videos:\n",
        "    TRAIN (0-20 actors)- /audio_video/train\n",
        "    TEST (21-24) - /audio_video/test\n",
        "  '''\n",
        "  dirs = os.listdir(src)\n",
        "  dirs.sort()\n",
        "  train_dirs = dirs[:n_samples]\n",
        "  test_dirs = dirs[n_samples:]\n",
        "\n",
        "  if not os.path.exists(TRAIN_DIR):\n",
        "    for cls in idx2class:\n",
        "      for dir in train_dirs:\n",
        "        os.makedirs(f'{TRAIN_DIR}/{cls}/{dir}', exist_ok=True)\n",
        "\n",
        "  if not os.path.exists(TEST_DIR):\n",
        "    for cls in idx2class:\n",
        "      for dir in test_dirs:\n",
        "        os.makedirs(f'{TEST_DIR}/{cls}/{dir}', exist_ok=True)\n",
        "\n",
        "  for dir in tqdm(train_dirs):\n",
        "    for file in os.listdir(f'{src}/{dir}'):\n",
        "      dst = f'{TRAIN_DIR}/{get_emotion(file)}/{dir}'\n",
        "      if 'None' not in dst and os.path.isfile(f'{src}/{dir}/{file}'):\n",
        "        shutil.move(f'{src}/{dir}/{file}', f'{dst}/{file}')\n",
        "    shutil.rmtree(f'{src}/{dir}')\n",
        "    \n",
        "  for dir in tqdm(test_dirs):\n",
        "    for file in os.listdir(f'{src}/{dir}'):\n",
        "      dst = f'{TEST_DIR}/{get_emotion(file)}/{dir}'\n",
        "      if 'None' not in dst and os.path.isfile(f'{src}/{dir}/{file}'):\n",
        "        shutil.move(f'{src}/{dir}/{file}', f'{dst}/{file}')\n",
        "    shutil.rmtree(f'{src}/{dir}')\n",
        "\n",
        "structure_dir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8294133c63194c23b8aae22abe3faf6f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf7723d52e24be4b806d8e6bd1bd3f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV_kSKA_14SC"
      },
      "source": [
        "!tree -d /content/audio_video/train/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOK65bnz4nr6"
      },
      "source": [
        "#### Feature Extraction\n",
        "\n",
        "Videos are just sequences of images (frames). Usually there are multiple frames in a second. Moreover, most of these frames don't have information that is significantly different. Also, the frames which that are redundant are usually the successive ones. Hence, we extract `n_frames` from each video and store them in their respective dir. Furthermore, it is a trick to reduce the complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_pwcut-XZof"
      },
      "source": [
        "def get_frames(src, n_frames=16):\n",
        "  '''\n",
        "  returns a list of evenly spaced n_frames extracted from each video \n",
        "  '''\n",
        "  frames_list = []\n",
        "  video = cv.VideoCapture(src)\n",
        "\n",
        "  # total frames\n",
        "  vframes = int(video.get(cv.CAP_PROP_FRAME_COUNT)) \n",
        "\n",
        "  # frames to be selected\n",
        "  frames_idx = np.linspace(0, vframes-1, n_frames+1, dtype=np.int16)\n",
        "  \n",
        "  for f in range(vframes):\n",
        "    (grabbed, frame) = video.read()\n",
        "\n",
        "    if not grabbed:\n",
        "      break\n",
        "\n",
        "    if f in frames_idx:\n",
        "      frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "      frames_list.append(frame)\n",
        "\n",
        "  video.release()\n",
        "  return frames_list\n",
        " \n",
        "\n",
        "def store_frames(frames_list, dst):\n",
        "  '''\n",
        "  stores a set of imgs. from the extracted n_frames of each video and\n",
        "  returns a list of frames with all the details\n",
        "  '''\n",
        "  video_name = os.path.basename(dst)\n",
        "  # print('video_name:',video_name)         # 02-01-05-01-01-02-09\n",
        "  \n",
        "  dst = os.path.dirname(dst)\n",
        "  # print('dst:',dst)                       # /content/audio_video/train/angry/Actor_12\n",
        "\n",
        "  emotion = get_emotion(video_name)\n",
        "\n",
        "  actor = os.path.basename(dst)\n",
        "\n",
        "  frames_dict = []\n",
        "\n",
        "  for ii, frame in enumerate(frames_list):\n",
        "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "    \n",
        "    path2img = f'{dst}/{video_name}_frame_{str(ii)}.jpg'\n",
        "    # print('path2img:',path2img)           # /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_0.jpg\n",
        "\n",
        "    dict4df = {'path2img':path2img, 'actor':actor, 'emotion':emotion, 'target':class2idx[emotion]}\n",
        "    \n",
        "    frames_dict.append(dict4df)\n",
        "    cv.imwrite(path2img, frame)\n",
        "  \n",
        "  return frames_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY2DOvPxliiN"
      },
      "source": [
        "def create_dataset(avtrain_df, avtest_df, n_frames=16):\n",
        "  '''\n",
        "  extract n_frames(imgs.) from each video and store them as jpg imgs.\n",
        "  '''\n",
        "  emotions = os.listdir(TRAIN_DIR)\n",
        "\n",
        "  for emotion in tqdm(emotions):\n",
        "    for actor in tqdm(os.scandir(f'{TRAIN_DIR}/{emotion}')):\n",
        "      for file in os.listdir(f'{TRAIN_DIR}/{emotion}/{actor.name}'):\n",
        "        filename = f'{TRAIN_DIR}/{emotion}/{actor.name}/{file}'\n",
        "        frames = get_frames(filename)\n",
        "        train_frames = store_frames(frames, os.path.splitext(filename)[0])\n",
        "        avtrain_df = avtrain_df.append(train_frames)\n",
        "        os.remove(filename)\n",
        "\n",
        "  emotions = os.listdir(TEST_DIR)\n",
        "\n",
        "  for emotion in tqdm(emotions):\n",
        "    for actor in tqdm(os.scandir(f'{TEST_DIR}/{emotion}')):\n",
        "      for file in os.listdir(f'{TEST_DIR}/{emotion}/{actor.name}'):\n",
        "        filename = f'{TEST_DIR}/{emotion}/{actor.name}/{file}'\n",
        "        frames = get_frames(filename)\n",
        "        test_frames = store_frames(frames, os.path.splitext(filename)[0])\n",
        "        avtest_df = avtest_df.append(test_frames)\n",
        "        os.remove(filename)\n",
        "\n",
        "  return avtrain_df, avtest_df\n",
        "\n",
        "train_df = pd.DataFrame(columns=['path2img', 'target', 'emotion'])\n",
        "test_df = pd.DataFrame(columns=['path2img', 'target', 'emotion'])\n",
        "\n",
        "train_df, test_df = create_dataset(avtrain_df=train_df, avtest_df=test_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIY1A1f_TAm"
      },
      "source": [
        "#### Minimal EDA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlnxHwKVh9zp",
        "outputId": "b13c5c0c-adb4-47c2-9fbe-f674f1a53a35"
      },
      "source": [
        "# sanity check\n",
        "print(train_df.shape)\n",
        "\n",
        "print(test_df.shape)\n",
        "\n",
        "# save as csv file\n",
        "train_df.to_csv('/content/drive/MyDrive/HMI_backup/train.csv')\n",
        "\n",
        "test_df.to_csv('/content/drive/MyDrive/HMI_backup/test.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19920, 4)\n",
            "(4080, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "cea3i83EPZWx",
        "outputId": "ba3abaee-e2b4-478c-c517-607f0b71ce43"
      },
      "source": [
        "def plot_distribution(df=train_df):\n",
        "  ''' plots a bar chart indication the distribution of emotions'''\n",
        "\n",
        "  plt.style.use('ggplot')\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  samples = df.groupby(['emotion'])['target'].agg('count')\n",
        "  print(samples)\n",
        "\n",
        "  data = [sample for sample in samples]\n",
        "  labels = sorted(df['emotion'].unique())\n",
        "  ax.set_title(f\"Distribution of different emotion samples in the training set\")\n",
        "  \n",
        "  ax.barh(labels, data, height=0.6)\n",
        "  ax.set_xlabel('Training Samples')\n",
        "  ax.set_ylabel('Emotion')\n",
        "  ax.set_yticklabels(labels)\n",
        "      \n",
        "  plt.show()\n",
        "\n",
        "plot_distribution()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emotion\n",
            "angry        2656\n",
            "calm         2656\n",
            "disgust      2656\n",
            "fearful      2656\n",
            "happy        2656\n",
            "neutral      1328\n",
            "sad          2656\n",
            "surprised    2656\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nroQrgog4RB4"
      },
      "source": [
        "The above bar plot shows that the dataset is quite balanced. This helps us to estabilish that even Classification as an evaluation metric would be suitable. Also, we are relieved of handling the data imbalance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfecz8ii2Evp",
        "outputId": "3da70056-7e97-46b6-9424-9ed4b757a301"
      },
      "source": [
        "def train_val_split(df=train_df, split=0.2):\n",
        "  ''' splits the training set into 80:20 (default) and returns a val_df '''\n",
        "\n",
        "  actors = df['actor'].unique().tolist()\n",
        "  actors.sort()\n",
        "\n",
        "  val_size = int(split*len(actors))\n",
        "  # print(val_size)\n",
        "\n",
        "  val_actors = actors[-val_size:]\n",
        "  print(val_actors)\n",
        "\n",
        "  val_df = df.loc[df['actor'].isin(val_actors)]\n",
        "\n",
        "  return val_df\n",
        "\n",
        "val_df = train_val_split()\n",
        "print(val_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Actor_17', 'Actor_18', 'Actor_19', 'Actor_20']\n",
            "(3960, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "qFoJa1W8bXCv",
        "outputId": "2d9be06d-de5e-4616-8f4d-e58af346168d"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path2img</th>\n",
              "      <th>target</th>\n",
              "      <th>emotion</th>\n",
              "      <th>actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_0.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_1.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_2.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_3.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_4.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     path2img  ...     actor\n",
              "0  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_0.jpg  ...  Actor_12\n",
              "1  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_1.jpg  ...  Actor_12\n",
              "2  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_2.jpg  ...  Actor_12\n",
              "3  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_3.jpg  ...  Actor_12\n",
              "4  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_4.jpg  ...  Actor_12\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "_vTqX3BW2Uzn",
        "outputId": "747b006f-574a-45f1-bb75-43387aacf7f8"
      },
      "source": [
        "val_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path2img</th>\n",
              "      <th>target</th>\n",
              "      <th>emotion</th>\n",
              "      <th>actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_0.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_1.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_2.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_3.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_4.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     path2img  ...     actor\n",
              "0  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_0.jpg  ...  Actor_18\n",
              "1  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_1.jpg  ...  Actor_18\n",
              "2  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_2.jpg  ...  Actor_18\n",
              "3  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_3.jpg  ...  Actor_18\n",
              "4  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_4.jpg  ...  Actor_18\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "xhIYv2JXbbkd",
        "outputId": "7308415d-4f64-4bf6-957a-33523842d1c1"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path2img</th>\n",
              "      <th>target</th>\n",
              "      <th>emotion</th>\n",
              "      <th>actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_0.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_1.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_2.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_3.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_4.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                    path2img  ...     actor\n",
              "0  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_0.jpg  ...  Actor_24\n",
              "1  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_1.jpg  ...  Actor_24\n",
              "2  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_2.jpg  ...  Actor_24\n",
              "3  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_3.jpg  ...  Actor_24\n",
              "4  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_4.jpg  ...  Actor_24\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1lIh-MXbjg"
      },
      "source": [
        "# transforms for the dataset\n",
        "train_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                       transforms.Resize((224,224)),\n",
        "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                       transforms.RandomVerticalFlip(p=0.5),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                            [0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                      transforms.Resize((224,224)),                                      \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                          [0.229, 0.224, 0.225])                         \n",
        "                                   ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_SAGjUh8Com"
      },
      "source": [
        "After observing the dataset, we could conclude that not only different emotions need to be classified but also the actors shouldn't be mixed up. Hence, we need to calculate valid frames per emotion per actor. The Sampler ensures that only valid frames are selected from the complete dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG32Qfx51qdx"
      },
      "source": [
        "def get_frame_idx(df=train_df):\n",
        "  '''\n",
        "  returns a list of end frame indices for each emotion n actor   \n",
        "  '''\n",
        "  frame_idx = [0]\n",
        "\n",
        "  idx = df.groupby(['emotion', 'actor'])['path2img'].count().values\n",
        "  idx = idx.tolist()\n",
        "\n",
        "  frame_idx.extend(idx)\n",
        "\n",
        "  return torch.cumsum(torch.tensor(frame_idx), dim=0)\n",
        "\n",
        "\n",
        "class AVSampler(Sampler):\n",
        "  '''Custom Sampling Class for selection of valid frames'''\n",
        "  def __init__(self, frame_idx, n_frames):\n",
        "    frames = []\n",
        "    for i in range(len(frame_idx)-1):\n",
        "      start = frame_idx[i] \n",
        "      end = frame_idx[i+1] - n_frames\n",
        "      frames.append(torch.arange(start, end))\n",
        "    frames = torch.cat(frames)\n",
        "    self.frames = frames\n",
        "\n",
        "  def __iter__(self):\n",
        "    frames = self.frames[torch.randperm(len(self.frames))]\n",
        "    return iter(frames.tolist())\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.frames)\n",
        "\n",
        "\n",
        "class AVDataset(Dataset):\n",
        "  ''' Custom Dataset Class '''\n",
        "  def __init__(self, length, df=train_df, transform=train_transforms, n_frames=16):\n",
        "    self.length = length\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.n_frames = n_frames\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    ''' returns a data sample(img.) after applying transforms & the associated target(y_label) for a key(idx/index)'''\n",
        "    frames_start = idx\n",
        "    frames_end = idx + self.n_frames\n",
        "    idxs = list(range(frames_start, frames_end))\n",
        "\n",
        "    imgs = []\n",
        "    for i in idxs:\n",
        "      img_path = self.df.iloc[i]['path2img'] \n",
        "      img = cv.imread(img_path)\n",
        "      img = self.transform(img)\n",
        "      imgs.append(img)\n",
        "\n",
        "    imgs = torch.stack(imgs)\n",
        "    y_label = torch.tensor(self.df.iloc[frames_start]['target'], dtype=torch.long)\n",
        "    \n",
        "    return (imgs, y_label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7KId2d671Pz"
      },
      "source": [
        "# HYPERPARAMS\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1\n",
        "LR = 1e-3\n",
        "PATIENCE = 3\n",
        "FACTOR = 0.1\n",
        "\n",
        "# valid frame indices\n",
        "train_frame_idx = get_frame_idx(df=train_df)\n",
        "val_frame_idx = get_frame_idx(df=val_df)\n",
        "test_frame_idx = get_frame_idx(df=test_df)\n",
        "\n",
        "train_sampler = AVSampler(train_frame_idx, n_frames=16)\n",
        "val_sampler = AVSampler(val_frame_idx, n_frames=16)\n",
        "test_sampler = AVSampler(test_frame_idx, n_frames=16)\n",
        "\n",
        "train_data = AVDataset(length=len(train_sampler), df=train_df, transform=train_transforms)\n",
        "val_data = AVDataset(length=len(val_sampler), df=val_df, transform=test_transforms)\n",
        "test_data = AVDataset(length=len(test_sampler), df=test_df, transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, sampler=train_sampler, shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, sampler=test_sampler, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38-chPg45lzM",
        "outputId": "c0fb3348-3079-483e-8c0e-e5050ebdd4c5"
      },
      "source": [
        "# sanity check\n",
        "\n",
        "# obtain a batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 16, 3, 224, 224])\n",
            "torch.Size([32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk_B0YBWzwsu"
      },
      "source": [
        "#### Approach, Model Architecture and Traning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGKD-V12-Ki6"
      },
      "source": [
        "Different approaches are available to solve the problem ranging from hacky schemes (Single frame CNN) to 3D CNNs. Since the video clips were devoid of background features, we need to capitalize on the sequence. ConvNet combined with an LSTM would be a good choice as the ConvNet would be able to extract spatial features and the LSTM would extract the temporal features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wg-_YM4aIFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a1d463-fff7-40aa-a1f3-653d0dba35cc"
      },
      "source": [
        "class ResNet18_LSTM(nn.Module):\n",
        "  '''\n",
        "  returns a pretrained ConvNet (resnet18) + LSTM model\n",
        "  '''\n",
        "  def __init__(self, n_class, pretrained=True, freeze=True):\n",
        "    super(ResNet18_LSTM, self).__init__()\n",
        "    # pretrained resnet18\n",
        "    self.pretrainedmodel = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "    if freeze:\n",
        "      for param in self.pretrainedmodel.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # attach a linear classifier\n",
        "    self.pretrainedmodel.fc = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(self.pretrainedmodel.fc.in_features,256)),\n",
        "                          ]))\n",
        "    \n",
        "    # pass the features into an LSTM\n",
        "    self.lstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=2, batch_first=True)\n",
        "  \n",
        "    # linear classifier \n",
        "    # self.fc2 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=n_class)\n",
        "\n",
        "\n",
        "  def yolomod(self, model, brief=True):\n",
        "    ''' yolomod: you only look once at the model description'''\n",
        "    if brief:\n",
        "      print(summary(model))\n",
        "    else:\n",
        "      print(model)\n",
        "\n",
        "\n",
        "  def forward(self, frames):\n",
        "    bs, ts, c, h, w = frames.shape\n",
        "\n",
        "    final_state = None\n",
        "    for t in range(ts):\n",
        "      with torch.no_grad():\n",
        "        x = self.pretrainedmodel(frames[:, t, :, :, :])\n",
        "        \n",
        "        # print('features extracted:', x.shape)                                       # OUTPUT: [32, 256]\n",
        "        # print('Input 2 LSTM :', x.unsqueeze(0).shape)                               # OUTPUT: [1, 32, 256]\n",
        "        # print('Input 2 LSTM :', x.unsqueeze(1).shape)                               # OUTPUT: [32, 1, 256]\n",
        "\n",
        "        _output, final_state = self.lstm(x.unsqueeze(1), final_state)\n",
        "        # print('_output:', _output.shape)                                            # OUTPUT: [32, 1, 128]\n",
        "\n",
        "    # print('_output[-1, :, :]: ', _output[:, -1].shape)                              # OUTPUT: [32, 128]\n",
        "    # print('self.fc2(output[-1, :, :]): ', self.fc2(_output[-1, :, :]).shape)\n",
        "\n",
        "    # output = F.relu(self.fc2(_output[:, -1]))\n",
        "\n",
        "    # print('F.relu(self.fc2(_output[-1, :, :])): ', output.shape)\n",
        "    \n",
        "    output = self.fc2(_output[:, -1])\n",
        "    # print('output:', output.shape)                      # OUTPUT: [32, 8]\n",
        "    \n",
        "    return output    \n",
        "\n",
        "model = ResNet18_LSTM(n_class=8)\n",
        "# model.yolomod(model, brief=False)\n",
        "\n",
        "# initialize device as cuda if on GPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(f'on {device}!')\n",
        "  model.to(device)\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(f'on {device}!')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "on cuda:0!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_CIkKPYNkJ-"
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# specify scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='max', patience=PATIENCE, factor=FACTOR, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcqazUFpNJtv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02e4b370-3c17-4fb6-fc27-308b154b701d"
      },
      "source": [
        "# best_val_f1 = -np.Inf\n",
        "val_loss_min = np.Inf\n",
        "\n",
        "losses = {\n",
        "    'train_losses': [],\n",
        "    'val_losses': []\n",
        "}\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "  \n",
        "  train_loss, train_accuracy = 0.0, 0.0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for images, labels in train_loader:\n",
        "\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    scores = model(images)\n",
        "\n",
        "    loss = criterion(scores, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    train_predictions = F.softmax(scores, dim=1)\n",
        "\n",
        "    _, train_top_class = train_predictions.topk(1, dim=1)\n",
        "   \n",
        "    train_equals = train_top_class == labels.view(*train_top_class.shape)\n",
        "  \n",
        "    train_accuracy = torch.mean(train_equals.type(torch.FloatTensor))\n",
        "\n",
        "  else:\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      val_loss, val_accuracy = 0.0, 0.0\n",
        "\n",
        "      for images, labels in val_loader:\n",
        "        \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        scores = model(images)\n",
        "\n",
        "        loss = criterion(scores, labels)\n",
        "        \n",
        "        val_loss += loss.item()\n",
        "\n",
        "        val_predictions = F.softmax(scores, dim=1) \n",
        "\n",
        "        _, val_top_class = val_predictions.topk(1, dim=1)\n",
        "      \n",
        "        # calc. correct classes (Compare top_k and labels)\n",
        "        val_equals = val_top_class == labels.view(*val_top_class.shape)\n",
        "      \n",
        "        # calc. accuracy\n",
        "        val_accuracy = torch.mean(val_equals.type(torch.FloatTensor))\n",
        "\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    losses['train_losses'].append(train_loss)\n",
        "\n",
        "    val_loss = val_loss/len(val_loader) \n",
        "    losses['val_losses'].append(val_loss)\n",
        "\n",
        "    print(f'----- Epoch: {epoch}/{EPOCHS} ----- ')\n",
        "    print(f'Train Accuracy: {train_accuracy:.6f} || Validation Accuracy: {val_accuracy:.6f}')\n",
        "    print(f'Train Loss: {train_loss:.6f} || Val Loss: {val_loss:.6f}')\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # save model if validation loss has decreased\n",
        "    if val_loss <= val_loss_min:\n",
        "      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min, val_loss))\n",
        "      torch.save(model.state_dict(), '/content/mvc.pth')\n",
        "      val_loss_min = valid_loss\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-89b13ba4e289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-2d0f16b8fd2b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     54\u001b[0m       \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path2img'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m       \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \"\"\"\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    399\u001b[0m             )\n\u001b[1;32m    400\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;34m\"i.e. size should be an int or a sequence of length 1 in torchscript mode.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             )\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1903\u001b[0m                 )\n\u001b[1;32m   1904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}