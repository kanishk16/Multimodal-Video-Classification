{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal Video Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8294133c63194c23b8aae22abe3faf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aab819c578ed425cbf460199474c7b94",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_231ca45bf0b740ed95e417c99d443e47",
              "IPY_MODEL_a73a5288eee9491db0f604002306f8ed",
              "IPY_MODEL_f5e1455fcb724eaeb58de862e4018321"
            ]
          }
        },
        "aab819c578ed425cbf460199474c7b94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "231ca45bf0b740ed95e417c99d443e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9355fc0ec1384d4caa8581b66e2b8541",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa372152c2f54125abc089abfc3c4d97"
          }
        },
        "a73a5288eee9491db0f604002306f8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_acfded5535ac4c7aadcaf521fc4c00ce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 20,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_efd5ee412605489abde5e762c03ffc3b"
          }
        },
        "f5e1455fcb724eaeb58de862e4018321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_93dbc88e1609499d991995a6d40e59f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20/20 [00:02&lt;00:00,  4.91it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f0d24e5b3b324ead8382451fa638960f"
          }
        },
        "9355fc0ec1384d4caa8581b66e2b8541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa372152c2f54125abc089abfc3c4d97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "acfded5535ac4c7aadcaf521fc4c00ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "efd5ee412605489abde5e762c03ffc3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "93dbc88e1609499d991995a6d40e59f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f0d24e5b3b324ead8382451fa638960f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "adf7723d52e24be4b806d8e6bd1bd3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d9988b21a637457ba05bb1c04f572f34",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_31033da9c7a54d20966833cf9d813be0",
              "IPY_MODEL_782982516b3f47cb8e1602a97ca50d28",
              "IPY_MODEL_18c12abc1a3b49b99a6d627abd99ea25"
            ]
          }
        },
        "d9988b21a637457ba05bb1c04f572f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31033da9c7a54d20966833cf9d813be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab1bc4605fe54f398eb0716d6cc908e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0aced1e3ba94017bffc49d1bc5fc6ab"
          }
        },
        "782982516b3f47cb8e1602a97ca50d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3614b68d294e40f9be82b105a27a38e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 4,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8606a161b9a64a16aae91d58b7a3c78f"
          }
        },
        "18c12abc1a3b49b99a6d627abd99ea25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_06b96664547949ee95f4435fd187945d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4/4 [00:00&lt;00:00, 30.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67e9a00c3d4e46e692821786727d0cad"
          }
        },
        "ab1bc4605fe54f398eb0716d6cc908e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0aced1e3ba94017bffc49d1bc5fc6ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3614b68d294e40f9be82b105a27a38e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8606a161b9a64a16aae91d58b7a3c78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "06b96664547949ee95f4435fd187945d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67e9a00c3d4e46e692821786727d0cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49d780ede7f84361a8743a0afad8ed6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a8090a471ec246b58d731f98fe92cb5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_517e17d478dd4a568446c76aff0ab4cf",
              "IPY_MODEL_aa459a5eeff348b28af68d943b138a0d",
              "IPY_MODEL_e00249e9d32346e18bee17cdffd8256c"
            ]
          }
        },
        "a8090a471ec246b58d731f98fe92cb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "517e17d478dd4a568446c76aff0ab4cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28d8799665a14ff2b9c7abed6551e404",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82ecd84fdd4a4ed68b9fb6dc06a96b77"
          }
        },
        "aa459a5eeff348b28af68d943b138a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4578a5c590ff4c50ad43e683fb418e43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46830571,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46830571,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3d71b9c8f50469bbee1bb0e81afe3ac"
          }
        },
        "e00249e9d32346e18bee17cdffd8256c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb45f3929f3f42e2bca2158e7943356d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 89.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38a2d30ce20b4e7e991e987c9a4a04f8"
          }
        },
        "28d8799665a14ff2b9c7abed6551e404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82ecd84fdd4a4ed68b9fb6dc06a96b77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4578a5c590ff4c50ad43e683fb418e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a3d71b9c8f50469bbee1bb0e81afe3ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb45f3929f3f42e2bca2158e7943356d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38a2d30ce20b4e7e991e987c9a4a04f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98514bf39157433b8831b27de7dad581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_24964e06c6a348ba97113c702271995d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3bf6846bb5240d5bf6bfcb5d602fb08",
              "IPY_MODEL_4001849d25f640ad9bef1996a08b69e1"
            ]
          }
        },
        "24964e06c6a348ba97113c702271995d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3bf6846bb5240d5bf6bfcb5d602fb08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_14209d2c09274437913763a2e8413572",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.33MB of 0.33MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db1f90a299c54fa59b4bd010132f7030"
          }
        },
        "4001849d25f640ad9bef1996a08b69e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef84eb202d88438fa8381dcdce072f56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d4e04d0a487412e8463428e0e58a627"
          }
        },
        "14209d2c09274437913763a2e8413572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db1f90a299c54fa59b4bd010132f7030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef84eb202d88438fa8381dcdce072f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d4e04d0a487412e8463428e0e58a627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2c84b5c306c14582974adab2bf959a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0fe3243f7ac6417ab5fec220caa556b2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab9d03896e9949eaa87f568fcae0f04e",
              "IPY_MODEL_85ca7316b8e844f394f1f82c23a71812",
              "IPY_MODEL_52cfd645fb3f4d759455e39512860f8a"
            ]
          }
        },
        "0fe3243f7ac6417ab5fec220caa556b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab9d03896e9949eaa87f568fcae0f04e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_64c7f9c26ab949fdbb82a655fb49a87f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 98%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9ecb16354234e63ad6cb852ff4b2cca"
          }
        },
        "85ca7316b8e844f394f1f82c23a71812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_755228d132ca428f9134ba316638c640",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 272,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe47d4d6fdb444bdaf95d39bce4438eb"
          }
        },
        "52cfd645fb3f4d759455e39512860f8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3cc3be2ead2a41628c76c7ad696bd973",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 267/272 [1:22:26&lt;01:34, 18.82s/it, f1=tensor(0.4531)]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e7d177b70a945bfb23edcdba1e3b70b"
          }
        },
        "64c7f9c26ab949fdbb82a655fb49a87f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9ecb16354234e63ad6cb852ff4b2cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "755228d132ca428f9134ba316638c640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe47d4d6fdb444bdaf95d39bce4438eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3cc3be2ead2a41628c76c7ad696bd973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e7d177b70a945bfb23edcdba1e3b70b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanishk16/Multimodal-Video-Classification/blob/master/Multimodal_Video_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJXlv2ZQC221"
      },
      "source": [
        "### A brief about Challenge\n",
        "\n",
        "* Problem Statement\n",
        "  > Paper ref. - https://sci-hub.se/10.1145/2818346.2829994\n",
        "\n",
        "  > To assign a single emotion label to the video clip from the eight universal emotions (Calm, Happy, Sad, Angry, Fearful, Disgust, Surprised and Neutral) on the given multimodal dataset. Use the first 20 actors for the training and rest(04) for testing.\n",
        "\n",
        "* Approach\n",
        "\n",
        "  > ConvNet (Spatial features) + LSTM (temporal features)\n",
        "  \n",
        "* Metrics\n",
        "  > Confusion Matrix + Precision + Recall + F1 score\n",
        "\n",
        "* References - \n",
        "1. The Ryerson Audio-Visual Database of Emotional Speech and Song(RAVDESS) Dataset - https://zenodo.org/record/1188976\n",
        "2. https://discuss.pytorch.org/t/how-upload-sequence-of-image-on-video-classification/24865 \n",
        "3. https://discuss.pytorch.org/t/solved-concatenate-time-distributed-cnn-with-lstm/15435\n",
        "4. https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/#heading4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBN97oSGJZ_W"
      },
      "source": [
        "### Installing additional packages & importing necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbEvQL5JJYxa"
      },
      "source": [
        "%%capture\n",
        "!apt-get install tree\n",
        "\n",
        "!pip install -q torchinfo\n",
        "!pip install -q torchmetrics\n",
        "!pip install -q gputil\n",
        "!pip install -q psutil\n",
        "!pip install -q humanize\n",
        "!pip install -q wandb\n",
        "\n",
        "import os\n",
        "import time\n",
        "import io\n",
        "import zipfile\n",
        "import psutil\n",
        "import humanize\n",
        "import GPUtil\n",
        "from collections import OrderedDict\n",
        "import shutil\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "from torchinfo import summary\n",
        "from torchmetrics import Precision, Recall, F1, ConfusionMatrix, Accuracy\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from torchvision import datasets, models, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aLZ0fY1y1rU"
      },
      "source": [
        "### CPU n GPU Footprint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfyskt5Yy0s1",
        "outputId": "8fb8def2-1727-4860-e1a9-4ee3e3c35f1d"
      },
      "source": [
        "def memory_report(cpu=True, gpu=True, verbose=False):\n",
        "  ''' CPU n GPU footprint '''\n",
        "\n",
        "  if cpu:\n",
        "    print(f'---------- CPU ----------')\n",
        "    if verbose:\n",
        "      !lscpu\n",
        "\n",
        "    print(f'RAM: {humanize.naturalsize(psutil.virtual_memory().available)}') \n",
        "    print(f'Cores x threads per core: {psutil.cpu_count()}')\n",
        "    print(f'Utilisation: {psutil.virtual_memory().percent} %')    # ~ psutil.virtual_memory().used/psutil.virtual_memory().available*100\n",
        "\n",
        "  if gpu:\n",
        "    print(f'---------- GPU ----------')\n",
        "\n",
        "    if verbose:\n",
        "      !nvidia-smi\n",
        "\n",
        "    # Method I: PyTorch leading the way\n",
        "    gpu = torch.cuda.get_device_properties('cuda:0')\n",
        "    gpus = GPUtil.getGPUs()[0]\n",
        "\n",
        "    print(f'GPU Allocated: {gpu.name}')\n",
        "    print(f'RAM: {gpus.memoryUsed/2**10 :.2f}/{gpu.total_memory/(2**30) :.2f} GB')\n",
        "    # print(f'Utilisation: {(torch.cuda.memory_allocated()/2**20)/gpu.total_memory*100 :.2f} %')  # OUTPUT: ~ WRONG!!\n",
        "    print(f'Utilisation: {gpus.memoryUtil*100 :.2f} %')\n",
        "    \n",
        "    # Method II: using an external package (GPUtil) \n",
        "    # gpus = GPUtil.getGPUs()\n",
        "    # for id, gpu in enumerate(gpus):\n",
        "    #   print(f'RAM: {gpu.memoryFree/1024 :.2f}/{gpu.memoryTotal/1024 :.2f}')\n",
        "    #   print(f'Utilisation: {gpu.memoryUtil*100 :.2f}({gpu.memoryUsed/1024 :.2f})')\n",
        "\n",
        "memory_report()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- CPU ----------\n",
            "RAM: 12.7 GB\n",
            "Cores x threads per core: 2\n",
            "Utilisation: 7.0 %\n",
            "---------- GPU ----------\n",
            "GPU Allocated: Tesla K80\n",
            "RAM: 0.00/11.17 GB\n",
            "Utilisation: 0.03 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB5oTg621Jl5"
      },
      "source": [
        "### Setting Up the Dataset\n",
        "\n",
        "> The format of the dataset provided to us is in the form of a zip file, we unzip and restructure into a train and test dir. After initial restructuring the dataset is stored:\n",
        "\n",
        "```\n",
        "* from: /content/audio_video/\n",
        "                            ├── Actor_01 \n",
        "                            ├── Actor_02 \n",
        "                            ├── Actor_03 \n",
        "                            ├── Actor_04\n",
        "                                  ...\n",
        "\n",
        "* to:/content/audio_video/train/\n",
        "                                ├── angry\n",
        "                                │   ├── Actor_01\n",
        "                                │   ├── Actor_02\n",
        "                                    ...\n",
        "                                ├── calm\n",
        "                                │   ├── Actor_01\n",
        "                                │   ├── Actor_02\n",
        "                                    ... \n",
        "\n",
        "```\n",
        "\n",
        "Similarly for test dir ...\n",
        "\n",
        "After initial observation, we realize that not only emotions have to be taken care of but even the actors can't be mixed up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooUnmH0vnrbi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "87118980-3cf1-4e48-eb59-9cf136ee43bd"
      },
      "source": [
        "def initial_setup(mount_drive=True, initialise_wandb=True, reproduce=True, seed=2021):\n",
        "  '''\n",
        "  The Project Agnostic Setup: Mount Google Drive + Login to W&B + set seeds for reproducibility \n",
        "  '''\n",
        "  # mount google drive\n",
        "  if mount_drive and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  # setup W&B\n",
        "  if initialise_wandb:\n",
        "    wandb.login()\n",
        "\n",
        "  # set seeds to reproduce the results\n",
        "  if reproduce:\n",
        "    np.random.seed(seed)\n",
        "    torch.manual.seed(seed)\n",
        "\n",
        "initial_setup(initialise_wandb=True, reproduce=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIJMuzQBsuoe"
      },
      "source": [
        "def download_dataset(filename='/content/drive/MyDrive/HMI/audio-video.zip',\n",
        "                     dirname='/content/audio_video/',\n",
        "                     preprocessed=True):\n",
        "  ''' \n",
        "  Recursively unzip the dataset to their respective dirs inside a dir passed as an arg\n",
        "  '''\n",
        "  \n",
        "  if preprocessed:\n",
        "    \n",
        "    !unzip -q /content/drive/MyDrive/HMI_backup/audio_video.zip\n",
        "    shutil.move('/content/content/audio_video', '/content/audio_video')\n",
        "    shutil.rmtree('/content/content')\n",
        "\n",
        "    train_df = pd.read_csv('/content/drive/MyDrive/HMI_backup/train.csv', index_col=0)\n",
        "    test_df = pd.read_csv('/content/drive/MyDrive/HMI_backup/test.csv', index_col=0)\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "  else:\n",
        "    with zipfile.ZipFile(filename) as zip_files:\n",
        "      for zip_file in tqdm(zip_files.namelist()[1:]):\n",
        "        data = io.BytesIO(zip_files.read(zip_file))\n",
        "        _zip_file = zipfile.ZipFile(data)\n",
        "        _zip_file.extractall(dirname)\n",
        "\n",
        "train_df, test_df = download_dataset(preprocessed=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLU0J_l6U0dK"
      },
      "source": [
        "# dir structure\n",
        "!tree -d /content/audio_video/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93-5eunon99b"
      },
      "source": [
        "# defining train and test dir paths\n",
        "TRAIN_DIR = '/content/audio_video/train'\n",
        "TEST_DIR = '/content/audio_video/test'\n",
        "\n",
        "idx2class = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
        "class2idx = {cls:idx for idx, cls in enumerate(idx2class)}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vD3rhmrnayR"
      },
      "source": [
        "def get_emotion(file):\n",
        "  ''' \n",
        "  returns the emotion based upon the filename\n",
        "  '''\n",
        "  if file.startswith('02-01-01'):\n",
        "    return f'neutral'\n",
        "  elif file.startswith('02-01-02'):\n",
        "    return f'calm'\n",
        "  elif file.startswith('02-01-03'):\n",
        "    return f'happy'\n",
        "  elif file.startswith('02-01-04'):\n",
        "    return f'sad'\n",
        "  elif file.startswith('02-01-05'):\n",
        "    return f'angry'\n",
        "  elif file.startswith('02-01-06'):\n",
        "    return f'fearful'\n",
        "  elif file.startswith('02-01-07'):\n",
        "    return f'disgust'\n",
        "  elif file.startswith('02-01-08'):\n",
        "    return f'surprised'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8294133c63194c23b8aae22abe3faf6f",
            "aab819c578ed425cbf460199474c7b94",
            "231ca45bf0b740ed95e417c99d443e47",
            "a73a5288eee9491db0f604002306f8ed",
            "f5e1455fcb724eaeb58de862e4018321",
            "9355fc0ec1384d4caa8581b66e2b8541",
            "aa372152c2f54125abc089abfc3c4d97",
            "acfded5535ac4c7aadcaf521fc4c00ce",
            "efd5ee412605489abde5e762c03ffc3b",
            "93dbc88e1609499d991995a6d40e59f1",
            "f0d24e5b3b324ead8382451fa638960f",
            "adf7723d52e24be4b806d8e6bd1bd3f8",
            "d9988b21a637457ba05bb1c04f572f34",
            "31033da9c7a54d20966833cf9d813be0",
            "782982516b3f47cb8e1602a97ca50d28",
            "18c12abc1a3b49b99a6d627abd99ea25",
            "ab1bc4605fe54f398eb0716d6cc908e7",
            "c0aced1e3ba94017bffc49d1bc5fc6ab",
            "3614b68d294e40f9be82b105a27a38e7",
            "8606a161b9a64a16aae91d58b7a3c78f",
            "06b96664547949ee95f4435fd187945d",
            "67e9a00c3d4e46e692821786727d0cad"
          ]
        },
        "id": "wVTP5ql2aEcF",
        "outputId": "4901b267-7d09-4ae5-aede-11259312d2c0"
      },
      "source": [
        "def structure_dir(src='/content/audio_video', n_samples=20):\n",
        "  '''\n",
        "    returns a reorganized directory structure by splitting the dataset into TRAIN/TEST and includes only videos:\n",
        "    TRAIN (0-20 actors)- /audio_video/train\n",
        "    TEST (21-24) - /audio_video/test\n",
        "  '''\n",
        "  dirs = os.listdir(src)\n",
        "  dirs.sort()\n",
        "  train_dirs = dirs[:n_samples]\n",
        "  test_dirs = dirs[n_samples:]\n",
        "\n",
        "  if not os.path.exists(TRAIN_DIR):\n",
        "    for cls in idx2class:\n",
        "      for dir in train_dirs:\n",
        "        os.makedirs(f'{TRAIN_DIR}/{cls}/{dir}', exist_ok=True)\n",
        "\n",
        "  if not os.path.exists(TEST_DIR):\n",
        "    for cls in idx2class:\n",
        "      for dir in test_dirs:\n",
        "        os.makedirs(f'{TEST_DIR}/{cls}/{dir}', exist_ok=True)\n",
        "\n",
        "  for dir in tqdm(train_dirs):\n",
        "    for file in os.listdir(f'{src}/{dir}'):\n",
        "      dst = f'{TRAIN_DIR}/{get_emotion(file)}/{dir}'\n",
        "      if 'None' not in dst and os.path.isfile(f'{src}/{dir}/{file}'):\n",
        "        shutil.move(f'{src}/{dir}/{file}', f'{dst}/{file}')\n",
        "    shutil.rmtree(f'{src}/{dir}')\n",
        "    \n",
        "  for dir in tqdm(test_dirs):\n",
        "    for file in os.listdir(f'{src}/{dir}'):\n",
        "      dst = f'{TEST_DIR}/{get_emotion(file)}/{dir}'\n",
        "      if 'None' not in dst and os.path.isfile(f'{src}/{dir}/{file}'):\n",
        "        shutil.move(f'{src}/{dir}/{file}', f'{dst}/{file}')\n",
        "    shutil.rmtree(f'{src}/{dir}')\n",
        "\n",
        "structure_dir()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8294133c63194c23b8aae22abe3faf6f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adf7723d52e24be4b806d8e6bd1bd3f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV_kSKA_14SC"
      },
      "source": [
        "!tree -d /content/audio_video/train/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOK65bnz4nr6"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "Videos are just sequences of images (frames). Usually there exists multiple frames in a second. Moreover, most of these frames don't comprise information that is significantly different. Also, the frames which that are redundant are usually the successive ones. Hence, we extract `n_frames` from each video and store them in their respective dir. Furthermore, it is a trick to reduce the complexity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_pwcut-XZof"
      },
      "source": [
        "def get_frames(src, n_frames=16):\n",
        "  '''\n",
        "  returns a list of evenly spaced n_frames extracted from each video \n",
        "  '''\n",
        "  frames_list = []\n",
        "  video = cv.VideoCapture(src)\n",
        "\n",
        "  # total frames\n",
        "  vframes = int(video.get(cv.CAP_PROP_FRAME_COUNT)) \n",
        "\n",
        "  # frames to be selected\n",
        "  frames_idx = np.linspace(0, vframes-1, n_frames+1, dtype=np.int16)\n",
        "  \n",
        "  for f in range(vframes):\n",
        "    (grabbed, frame) = video.read()\n",
        "\n",
        "    if not grabbed:\n",
        "      break\n",
        "\n",
        "    if f in frames_idx:\n",
        "      frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "      frames_list.append(frame)\n",
        "\n",
        "  video.release()\n",
        "  return frames_list\n",
        " \n",
        "\n",
        "def store_frames(frames_list, dst):\n",
        "  '''\n",
        "  stores a set of imgs. from the extracted n_frames of each video and\n",
        "  returns a list of frames with all the details\n",
        "  '''\n",
        "  video_name = os.path.basename(dst)\n",
        "  # print('video_name:',video_name)         # 02-01-05-01-01-02-09\n",
        "  \n",
        "  dst = os.path.dirname(dst)\n",
        "  # print('dst:',dst)                       # /content/audio_video/train/angry/Actor_12\n",
        "\n",
        "  emotion = get_emotion(video_name)\n",
        "\n",
        "  actor = os.path.basename(dst)\n",
        "\n",
        "  frames_dict = []\n",
        "\n",
        "  for ii, frame in enumerate(frames_list):\n",
        "    frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
        "    \n",
        "    path2img = f'{dst}/{video_name}_frame_{str(ii)}.jpg'\n",
        "    # print('path2img:',path2img)           # /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_0.jpg\n",
        "\n",
        "    dict4df = {'path2img':path2img, 'actor':actor, 'emotion':emotion, 'target':class2idx[emotion]}\n",
        "    \n",
        "    frames_dict.append(dict4df)\n",
        "    cv.imwrite(path2img, frame)\n",
        "  \n",
        "  return frames_dict\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY2DOvPxliiN"
      },
      "source": [
        "def create_dataset(avtrain_df, avtest_df, n_frames=16):\n",
        "  '''\n",
        "  extract n_frames(imgs.) from each video and store them as jpg imgs.\n",
        "  '''\n",
        "  emotions = os.listdir(TRAIN_DIR)\n",
        "\n",
        "  for emotion in tqdm(emotions):\n",
        "    for actor in tqdm(os.scandir(f'{TRAIN_DIR}/{emotion}')):\n",
        "      for file in os.listdir(f'{TRAIN_DIR}/{emotion}/{actor.name}'):\n",
        "        filename = f'{TRAIN_DIR}/{emotion}/{actor.name}/{file}'\n",
        "        frames = get_frames(filename)\n",
        "        train_frames = store_frames(frames, os.path.splitext(filename)[0])\n",
        "        avtrain_df = avtrain_df.append(train_frames)\n",
        "        os.remove(filename)\n",
        "\n",
        "  emotions = os.listdir(TEST_DIR)\n",
        "\n",
        "  for emotion in tqdm(emotions):\n",
        "    for actor in tqdm(os.scandir(f'{TEST_DIR}/{emotion}')):\n",
        "      for file in os.listdir(f'{TEST_DIR}/{emotion}/{actor.name}'):\n",
        "        filename = f'{TEST_DIR}/{emotion}/{actor.name}/{file}'\n",
        "        frames = get_frames(filename)\n",
        "        test_frames = store_frames(frames, os.path.splitext(filename)[0])\n",
        "        avtest_df = avtest_df.append(test_frames)\n",
        "        os.remove(filename)\n",
        "\n",
        "  return avtrain_df, avtest_df\n",
        "\n",
        "train_df = pd.DataFrame(columns=['path2img', 'target', 'emotion'])\n",
        "test_df = pd.DataFrame(columns=['path2img', 'target', 'emotion'])\n",
        "\n",
        "train_df, test_df = create_dataset(avtrain_df=train_df, avtest_df=test_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaIY1A1f_TAm"
      },
      "source": [
        "### Minimal EDA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlnxHwKVh9zp",
        "outputId": "31c3ad51-7e72-40a8-a979-21185c8f287a"
      },
      "source": [
        "# sanity check\n",
        "print(train_df.shape)\n",
        "\n",
        "print(test_df.shape)\n",
        "\n",
        "# save as csv file\n",
        "# train_df.to_csv('/content/drive/MyDrive/HMI_backup/train.csv')\n",
        "\n",
        "# test_df.to_csv('/content/drive/MyDrive/HMI_backup/test.csv')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19920, 4)\n",
            "(4080, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "cea3i83EPZWx",
        "outputId": "487b07f7-ba8c-40d0-e02f-07adb95f46b0"
      },
      "source": [
        "def plot_distribution(df=train_df):\n",
        "  ''' plots a bar chart indication the distribution of emotions'''\n",
        "\n",
        "  plt.style.use('ggplot')\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  samples = df.groupby(['emotion'])['target'].agg('count')\n",
        "  print(samples)\n",
        "\n",
        "  data = [sample for sample in samples]\n",
        "  labels = sorted(df['emotion'].unique())\n",
        "  ax.set_title(f\"Distribution of different emotion samples in the training set\")\n",
        "  \n",
        "  ax.barh(labels, data, height=0.6)\n",
        "  ax.set_xlabel('Training Samples')\n",
        "  ax.set_ylabel('Emotion')\n",
        "  ax.set_yticklabels(labels)\n",
        "      \n",
        "  plt.show()\n",
        "\n",
        "plot_distribution()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emotion\n",
            "angry        2656\n",
            "calm         2656\n",
            "disgust      2656\n",
            "fearful      2656\n",
            "happy        2656\n",
            "neutral      1328\n",
            "sad          2656\n",
            "surprised    2656\n",
            "Name: target, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nroQrgog4RB4"
      },
      "source": [
        "The above bar plot shows that the dataset is quite balanced. This helps us to estabilish that even Classification as an evaluation metric would be suitable. Also, we are relieved of handling the data imbalance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfecz8ii2Evp",
        "outputId": "38fc8495-9e26-48c2-8df2-a939145582ef"
      },
      "source": [
        "def train_val_split(df=train_df, split=0.2):\n",
        "  ''' splits the training set into 80:20 (default) and returns a val_df '''\n",
        "\n",
        "  actors = df['actor'].unique().tolist()\n",
        "  actors.sort()\n",
        "\n",
        "  val_size = int(split*len(actors))\n",
        "  # print(val_size)\n",
        "\n",
        "  val_actors = actors[-val_size:]\n",
        "  print(val_actors)\n",
        "\n",
        "  val_df = df.loc[df['actor'].isin(val_actors)]\n",
        "\n",
        "  return val_df\n",
        "\n",
        "val_df = train_val_split()\n",
        "print(val_df.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Actor_17', 'Actor_18', 'Actor_19', 'Actor_20']\n",
            "(3960, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "qFoJa1W8bXCv",
        "outputId": "2dcf2679-4b73-4366-ba65-aed347592777"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path2img</th>\n",
              "      <th>target</th>\n",
              "      <th>emotion</th>\n",
              "      <th>actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_0.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_1.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_2.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_3.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_4.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     path2img  ...     actor\n",
              "0  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_0.jpg  ...  Actor_12\n",
              "1  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_1.jpg  ...  Actor_12\n",
              "2  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_2.jpg  ...  Actor_12\n",
              "3  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_3.jpg  ...  Actor_12\n",
              "4  /content/audio_video/train/angry/Actor_12/02-01-05-01-01-02-12_frame_4.jpg  ...  Actor_12\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "_vTqX3BW2Uzn",
        "outputId": "e109b4c1-ff03-49a9-ce90-dddb6a58dd88"
      },
      "source": [
        "val_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path2img</th>\n",
              "      <th>target</th>\n",
              "      <th>emotion</th>\n",
              "      <th>actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_0.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_1.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_2.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_3.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_4.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                     path2img  ...     actor\n",
              "0  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_0.jpg  ...  Actor_18\n",
              "1  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_1.jpg  ...  Actor_18\n",
              "2  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_2.jpg  ...  Actor_18\n",
              "3  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_3.jpg  ...  Actor_18\n",
              "4  /content/audio_video/train/angry/Actor_18/02-01-05-01-01-02-18_frame_4.jpg  ...  Actor_18\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "xhIYv2JXbbkd",
        "outputId": "246eac72-dcd9-4b5e-89c0-a41208e2cedc"
      },
      "source": [
        "test_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path2img</th>\n",
              "      <th>target</th>\n",
              "      <th>emotion</th>\n",
              "      <th>actor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_0.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_1.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_2.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_3.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_4.jpg</td>\n",
              "      <td>4</td>\n",
              "      <td>angry</td>\n",
              "      <td>Actor_24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                    path2img  ...     actor\n",
              "0  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_0.jpg  ...  Actor_24\n",
              "1  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_1.jpg  ...  Actor_24\n",
              "2  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_2.jpg  ...  Actor_24\n",
              "3  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_3.jpg  ...  Actor_24\n",
              "4  /content/audio_video/test/angry/Actor_24/02-01-05-01-01-02-24_frame_4.jpg  ...  Actor_24\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7940gCdo_k1y"
      },
      "source": [
        "### Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1lIh-MXbjg"
      },
      "source": [
        "# transforms for the dataset\n",
        "train_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                       transforms.Resize((224,224)),\n",
        "                                       transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                       transforms.RandomVerticalFlip(p=0.5),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                            [0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                      transforms.Resize((224,224)),                                      \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                                                          [0.229, 0.224, 0.225])                         \n",
        "                                   ])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_SAGjUh8Com"
      },
      "source": [
        "After observing the dataset, we could conclude that not only different emotions need to be classified but also the actors shouldn't be mixed up. Hence, we need to calculate valid frames per emotion per actor. The Sampler ensures that only valid frames are selected from the complete dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG32Qfx51qdx"
      },
      "source": [
        "def get_frame_idx(df=train_df):\n",
        "  '''\n",
        "  returns a list of end frame indices for each emotion n actor   \n",
        "  '''\n",
        "  frame_idx = [0]\n",
        "\n",
        "  idx = df.groupby(['emotion', 'actor'])['path2img'].count().values\n",
        "  idx = idx.tolist()\n",
        "\n",
        "  frame_idx.extend(idx)\n",
        "\n",
        "  return torch.cumsum(torch.tensor(frame_idx), dim=0)\n",
        "\n",
        "\n",
        "class AVSampler(Sampler):\n",
        "  '''Custom Sampling Class for selection of valid frames'''\n",
        "  def __init__(self, frame_idx, n_frames):\n",
        "    frames = []\n",
        "    for i in range(len(frame_idx)-1):\n",
        "      start = frame_idx[i] \n",
        "      end = frame_idx[i+1] - n_frames\n",
        "      frames.append(torch.arange(start, end))\n",
        "    frames = torch.cat(frames)\n",
        "    self.frames = frames\n",
        "\n",
        "  def __iter__(self):\n",
        "    frames = self.frames[torch.randperm(len(self.frames))]\n",
        "    return iter(frames.tolist())\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.frames)\n",
        "\n",
        "\n",
        "class AVDataset(Dataset):\n",
        "  ''' Custom Dataset Class '''\n",
        "  def __init__(self, length, df=train_df, transform=train_transforms, n_frames=16):\n",
        "    self.length = length\n",
        "    self.df = df\n",
        "    self.transform = transform\n",
        "    self.n_frames = n_frames\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    ''' returns a data sample(img.) after applying transforms & the associated target(y_label) for a key(idx/index)'''\n",
        "    frames_start = idx\n",
        "    frames_end = idx + self.n_frames\n",
        "    idxs = list(range(frames_start, frames_end))\n",
        "\n",
        "    imgs = []\n",
        "    for i in idxs:\n",
        "      img_path = self.df.iloc[i]['path2img'] \n",
        "      img = cv.imread(img_path)\n",
        "      img = self.transform(img)\n",
        "      imgs.append(img)\n",
        "\n",
        "    imgs = torch.stack(imgs)\n",
        "    y_label = torch.tensor(self.df.iloc[frames_start]['target'], dtype=torch.long)\n",
        "    \n",
        "    return (imgs, y_label)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxidQqllI29X"
      },
      "source": [
        "#### Speeding Up the Training Pipeline (Experiments)\n",
        "\n",
        "1. Batch Size: 64  |  num_workers: 0  |  pin_memory: False\n",
        "  > Time (per batch): 22.14s (batch 0) 20.76s (batch 5)\\\n",
        "  CPU Utilisation:  19.6 %  | GPU Utilisation: 4.57 -> 9.71 -> 64.79 \n",
        "\n",
        "2. Batch Size: 64  |  num_workers: 1  |  pin_memory: False\n",
        "  > Time (per batch): 22.82s (batch 0) 20.89s (batch 5)\\\n",
        "  CPU Utilisation:  19.9 %  | GPU Utilisation: 64.83 %\n",
        "\n",
        "3. Batch Size: 64  |  num_workers: 2  |  pin_memory: False\n",
        "  > Time (per batch): 39.08 $\\pm$ 3s (batch 0) 2.25s (batch 5)\\\n",
        "  CPU Utilisation:  24.5 %  | GPU Utilisation: 64.83 % \n",
        "\n",
        "4. (R) Batch Size: 64  |  num_workers: 2  |  pin_memory: True \n",
        "  > Time (per batch): 37.51 $\\pm$ 3s (batch 0) 1.86s (batch 5)\\\n",
        "  Time (per epoch): 1h 35m 16s\\\n",
        "  CPU Utilisation:  24.8 - 29.3 %  | GPU Utilisation: 64.86 % \n",
        "\n",
        "5. Batch Size: 64  |  num_workers: 4  |  pin_memory: True\n",
        "  > Time (per batch): 71.55 $\\pm$ s (batch 0) 1.87s (batch 5)\\\n",
        "  Time (per epoch): ~ 1h 45m\\\n",
        "  CPU Utilisation:  51.3 %  | GPU Utilisation: 64.88 % \n",
        "\n",
        "6. Batch Size: 64  |  num_workers: 8  |  pin_memory: True\n",
        "  > Time (per batch): 37.51s (batch 0) s (batch 5)\\\n",
        "  CPU Utilisation:  crash %  | GPU Utilisation: 64.86 % "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7KId2d671Pz"
      },
      "source": [
        "# HYPERPARAMS\n",
        "hyperparams = {\n",
        "    'BATCH_SIZE' : 64,\n",
        "    'EPOCHS' : 10,\n",
        "    'LR' : 1e-3,\n",
        "    'PATIENCE' : 3,\n",
        "    'FACTOR' : 0.1\n",
        "}\n",
        "\n",
        "train_frame_idx = get_frame_idx(df=train_df)\n",
        "val_frame_idx = get_frame_idx(df=val_df)\n",
        "test_frame_idx = get_frame_idx(df=test_df)\n",
        "\n",
        "train_sampler = AVSampler(train_frame_idx, n_frames=16)\n",
        "val_sampler = AVSampler(val_frame_idx, n_frames=16)\n",
        "test_sampler = AVSampler(test_frame_idx, n_frames=16)\n",
        "\n",
        "train_data = AVDataset(length=len(train_sampler), df=train_df, transform=train_transforms)\n",
        "val_data = AVDataset(length=len(val_sampler), df=val_df, transform=test_transforms)\n",
        "test_data = AVDataset(length=len(test_sampler), df=test_df, transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=hyperparams['BATCH_SIZE'], sampler=train_sampler, num_workers=2, pin_memory=True, shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_data, batch_size=hyperparams['BATCH_SIZE'], sampler=val_sampler, num_workers=2, pin_memory=True, shuffle=False)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=hyperparams['BATCH_SIZE'], sampler=test_sampler, num_workers=2, pin_memory=True, shuffle=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk_B0YBWzwsu"
      },
      "source": [
        "#### Approach, Model Architecture and Training\n",
        "\n",
        "Different approaches are available to solve the problem ranging from hacky schemes (Single frame CNN) to 3D CNNs. Since the video clips were devoid of background features, we need to capitalize on the sequence. ConvNet combined with an LSTM would be a good choice as the ConvNet would be able to extract spatial features and the LSTM would extract the temporal features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wg-_YM4aIFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "49d780ede7f84361a8743a0afad8ed6f",
            "a8090a471ec246b58d731f98fe92cb5d",
            "517e17d478dd4a568446c76aff0ab4cf",
            "aa459a5eeff348b28af68d943b138a0d",
            "e00249e9d32346e18bee17cdffd8256c",
            "28d8799665a14ff2b9c7abed6551e404",
            "82ecd84fdd4a4ed68b9fb6dc06a96b77",
            "4578a5c590ff4c50ad43e683fb418e43",
            "a3d71b9c8f50469bbee1bb0e81afe3ac",
            "fb45f3929f3f42e2bca2158e7943356d",
            "38a2d30ce20b4e7e991e987c9a4a04f8"
          ]
        },
        "outputId": "c7ce20c0-8f0a-42ac-abbc-db0e15bb8c38"
      },
      "source": [
        "class ResNet18_LSTM(nn.Module):\n",
        "  '''\n",
        "  returns a pretrained ConvNet (resnet18) + LSTM model\n",
        "  '''\n",
        "  def __init__(self, n_class, pretrained=True, freeze=True):\n",
        "    super(ResNet18_LSTM, self).__init__()\n",
        "    # pretrained resnet18\n",
        "    self.pretrainedmodel = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "    if freeze:\n",
        "      for param in self.pretrainedmodel.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # attach a linear classifier\n",
        "    self.pretrainedmodel.fc = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(self.pretrainedmodel.fc.in_features,256)),\n",
        "                          ]))\n",
        "    \n",
        "    # pass the features into an LSTM\n",
        "    self.lstm = nn.LSTM(input_size=256, hidden_size=128, num_layers=2, batch_first=True)\n",
        "  \n",
        "    # linear classifier \n",
        "    # self.fc2 = nn.Linear(in_features=128, out_features=128)\n",
        "    self.fc2 = nn.Linear(in_features=128, out_features=n_class)\n",
        "\n",
        "\n",
        "  def yolomod(self, model, brief=True):\n",
        "    ''' yolomod: you only look once at the model description'''\n",
        "    if brief:\n",
        "      print(summary(model))\n",
        "    else:\n",
        "      print(model)\n",
        "\n",
        "\n",
        "  def forward(self, frames, vectorized=True):\n",
        "    bs, ts, c, h, w = frames.shape\n",
        "\n",
        "    if vectorized:\n",
        "      \n",
        "      # print('frames.view(bs*ts, c, h, w):', frames.view(bs*ts, c, h, w).shape)    # OUTPUT: [512, 3, 224, 224]\n",
        "      x = self.pretrainedmodel(frames.view(bs*ts, c, h, w))                         \n",
        "      # print('features extracted:', x.shape)                                       # OUTPUT: [512, 256]      \n",
        "\n",
        "      # print('x.view(bs, ts, -1):', x.view(bs, ts, -1).shape)                      # OUTPUT: [32, 16, 256]\n",
        "      _output, (hn,cn) = self.lstm(x.view(bs, ts, -1))\n",
        "      # print('_output:', _output.shape)                                            # OUTPUT: [32, 16, 128]  \n",
        "\n",
        "      # print('_output:', _output[:, -1, :].shape)                                  # OUTPUT: [32, 128] using only the last states of lstm\n",
        "      output = self.fc2(_output[:, -1, :])\n",
        "      # print('output:', output.shape)                                              # OUTPUT: [32, 8]\n",
        "      \n",
        "      return output    \n",
        "\n",
        "    else:\n",
        "      final_state = None\n",
        "      for t in range(ts):\n",
        "        with torch.no_grad():\n",
        "          x = self.pretrainedmodel(frames[:, t, :, :, :])\n",
        "          \n",
        "          # print('features extracted:', x.shape)                                     # OUTPUT: [32, 256]\n",
        "          # print('Input2LSTM :', x.unsqueeze(0).shape)                               # OUTPUT: [1, 32, 256]\n",
        "          # print('Input2LSTM :', x.unsqueeze(1).shape)                               # OUTPUT: [32, 1, 256]\n",
        "\n",
        "        _output, final_state = self.lstm(x.unsqueeze(1), final_state)\n",
        "        # print('_output:', _output.shape)                                            # OUTPUT: [1, 32, 128] [32, 1, 128] \n",
        "\n",
        "      # print('_output[-1, :, :]: ', _output.squeeze().shape)                         # OUTPUT: [32, 128]\n",
        "      # print('self.fc2(output[-1, :, :]): ', self.fc2(_output[-1, :, :]).shape)\n",
        "\n",
        "      # output = F.relu(self.fc2(_output[:, -1]))\n",
        "\n",
        "      # print('F.relu(self.fc2(_output[-1, :, :])): ', output.shape)\n",
        "      \n",
        "      output = self.fc2(_output.squeeze())\n",
        "      # print('output:', output.shape)                      # OUTPUT: [32, 8]\n",
        "      \n",
        "      return output    \n",
        "\n",
        "model = ResNet18_LSTM(n_class=8)\n",
        "# model.yolomod(model, brief=False)\n",
        "\n",
        "# initialize device as cuda if on GPU...\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\")\n",
        "  print(f'on {device}!')\n",
        "  model.to(device)\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(f'on {device}!')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49d780ede7f84361a8743a0afad8ed6f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "on cuda:0!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n02Y04vj4K8h",
        "outputId": "c4dbaf48-09a0-4597-bc4c-a0396a80dc88"
      },
      "source": [
        "memory_report()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- CPU ----------\n",
            "RAM: 11.5 GB\n",
            "Cores x threads per core: 2\n",
            "Utilisation: 15.5 %\n",
            "---------- GPU ----------\n",
            "GPU Allocated: Tesla K80\n",
            "RAM: 0.51/11.17 GB\n",
            "Utilisation: 4.57 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_CIkKPYNkJ-"
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparams['LR'])\n",
        "\n",
        "# specify scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode='min', patience=hyperparams['PATIENCE'],\n",
        "                                                 factor=hyperparams['FACTOR'], verbose=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHyw4xkWgFUF"
      },
      "source": [
        "def save_checkpoint(epoch, model_state_dict, optim_state_dict, losses, accuracy,\n",
        "                    precision, recall, f1_score, filepath='/content/drive/MyDrive/HMI_backup/mmvc.pt'):\n",
        "  '''\n",
        "  Creates a checkpoint to restore the STATUS-QUO\n",
        "  '''\n",
        "  print('Saving Model ...')\n",
        "\n",
        "  checkpoint = {\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model_state_dict,\n",
        "      'optimizer_state_dict': optim_state_dict,\n",
        "      'scheduler_state_dict': scheduler_state_dict,\n",
        "      'losses': losses,\n",
        "      'accuracy': accuracy,\n",
        "      'precision': precision,\n",
        "      'recall': recall,\n",
        "      'f1_score': f1_score\n",
        "  }\n",
        "\n",
        "  torch.save(checkpoint, filepath)\n",
        "\n",
        "def load_model_checkpoint(filepath, model):\n",
        "  '''\n",
        "  Loads the saved model checkpoint dict\n",
        "  '''\n",
        "  checkpoint = torch.load(filepath)\n",
        "\n",
        "  print(f'Restoring the model state to epoch{checkpoint['epoch']}')\n",
        "\n",
        "  # model = ()\n",
        "  model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'], strict=False)\n",
        "\n",
        "def load_inference_checkpoint(filepath):\n",
        "  '''\n",
        "  Loads the saved checkpoint dict\n",
        "  '''\n",
        "  checkpoint = torch.load(filepath)\n",
        "\n",
        "  print(f'Restoring the losses state to epoch{checkpoint['epoch']}')\n",
        "  train_losses, val_losses, auc_score = checkpoint['train_losses'], checkpoint['val_losses'], checkpoint['auc_score']\n",
        "  return train_losses, val_losses, auc_score\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgiyK0rYvr-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302,
          "referenced_widgets": [
            "98514bf39157433b8831b27de7dad581",
            "24964e06c6a348ba97113c702271995d",
            "b3bf6846bb5240d5bf6bfcb5d602fb08",
            "4001849d25f640ad9bef1996a08b69e1",
            "14209d2c09274437913763a2e8413572",
            "db1f90a299c54fa59b4bd010132f7030",
            "ef84eb202d88438fa8381dcdce072f56",
            "5d4e04d0a487412e8463428e0e58a627"
          ]
        },
        "outputId": "4fd76446-323c-4f32-ceca-d91a46192d86"
      },
      "source": [
        "def track_experiments(group, job_type, project='multimodal video classification', config=hyperparams):\n",
        "  ''' Launch Experiment Tracking '''\n",
        "  \n",
        "  # initialize run\n",
        "  run = wandb.init(group=group, job_type=job_type, project=project, entity='kanish2k', config=config)\n",
        "  \n",
        "  return run\n",
        "\n",
        "run = track_experiments(group='exp_1', job_type='epochs_2')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:39l3ric6) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 965<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98514bf39157433b8831b27de7dad581",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.30MB of 0.30MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210830_051236-39l3ric6/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210830_051236-39l3ric6/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">ethereal-wave-2</strong>: <a href=\"https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/39l3ric6\" target=\"_blank\">https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/39l3ric6</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:39l3ric6). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.1<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">rare-shadow-3</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/kanish2k/multimodal%20video%20classification\" target=\"_blank\">https://wandb.ai/kanish2k/multimodal%20video%20classification</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/3sow0csf\" target=\"_blank\">https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/3sow0csf</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210830_051655-3sow0csf</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "MeuM0tG7sdAP",
        "outputId": "452a05fb-c5a7-44b7-9c54-b7f1c2a093a1"
      },
      "source": [
        "wandb.init(project='multimodal video classification', entity='kanish2k')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkanish2k\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.12.0<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">fragrant-valley-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/kanish2k/multimodal%20video%20classification\" target=\"_blank\">https://wandb.ai/kanish2k/multimodal%20video%20classification</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/24h3o2w2\" target=\"_blank\">https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/24h3o2w2</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210820_175656-24h3o2w2</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f8c868e0750>"
            ],
            "text/html": [
              "<h1>Run(24h3o2w2)</h1><iframe src=\"https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/24h3o2w2\" style=\"border:none;width:100%;height:400px\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W79tEtt2baMy"
      },
      "source": [
        "# initialize evaluation metrics\n",
        "losses = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "}\n",
        "\n",
        "train_precision = Precision()\n",
        "val_precision = Precision()\n",
        "precision = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "}\n",
        "\n",
        "\n",
        "train_recall = Recall()\n",
        "val_recall = Recall()\n",
        "recall ={\n",
        "    'train': [],\n",
        "    'val': []\n",
        "}\n",
        "\n",
        "\n",
        "train_f1 = F1()\n",
        "val_f1 = F1()\n",
        "f1_score = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "}\n",
        "\n",
        "train_accuracy = Accuracy()\n",
        "val_accuracy = Accuracy()\n",
        "accuracy = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "}\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543,
          "referenced_widgets": [
            "2c84b5c306c14582974adab2bf959a08",
            "0fe3243f7ac6417ab5fec220caa556b2",
            "ab9d03896e9949eaa87f568fcae0f04e",
            "85ca7316b8e844f394f1f82c23a71812",
            "52cfd645fb3f4d759455e39512860f8a",
            "64c7f9c26ab949fdbb82a655fb49a87f",
            "a9ecb16354234e63ad6cb852ff4b2cca",
            "755228d132ca428f9134ba316638c640",
            "fe47d4d6fdb444bdaf95d39bce4438eb",
            "3cc3be2ead2a41628c76c7ad696bd973",
            "6e7d177b70a945bfb23edcdba1e3b70b"
          ]
        },
        "id": "zwNoRyF_6a14",
        "outputId": "a459d920-f2a6-4038-c6d1-2d163d3f6a23"
      },
      "source": [
        "%%wandb\n",
        "val_loss_min = np.Inf\n",
        "\n",
        "# log_interval = 100\n",
        "\n",
        "for epoch in range(1, hyperparams['EPOCHS']+1):\n",
        "  \n",
        "  print(f\"---------- Epoch: {epoch}/{hyperparams['EPOCHS']} ----------\")\n",
        "  train_loss = 0.0\n",
        "\n",
        "  train_progress_bar = tqdm(train_loader)\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (images, labels) in enumerate(train_progress_bar):\n",
        "\n",
        "    # print('images: ', images.shape, 'labels: ', labels.shape)                   # OUTPUT: imgs: [32, 16, 3, 224, 224] labels: [32]\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    scores = model(images)\n",
        "\n",
        "    loss = criterion(scores, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    preds = F.softmax(scores, dim=1).cpu()\n",
        "    # print('preds:', preds.shape, 'on GPU:', preds.is_cuda)                      # OUTPUT: [32, 8] False\n",
        "\n",
        "    targets = labels.cpu()                                                      \n",
        "    # print('targets:', targets.shape, 'on GPU:', targets.is_cuda)                # OUTPUT: [32] False\n",
        "\n",
        "    # calc. metrics\n",
        "    _train_precision = train_precision(preds, targets)\n",
        "    _train_recall = train_recall(preds, targets)\n",
        "    _train_f1 = train_f1(preds, targets)\n",
        "    _train_accuracy = train_accuracy(preds, targets)\n",
        "\n",
        "    train_progress_bar.set_postfix(f1=_train_f1.item())\n",
        "\n",
        "  else:\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      val_loss = 0.0\n",
        "\n",
        "      val_progress_bar = tqdm(val_loader)\n",
        "\n",
        "      for images, labels in enumerate(val_progress_bar):\n",
        "        \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        scores = model(images)\n",
        "\n",
        "        loss = criterion(scores, labels)\n",
        "        \n",
        "        val_loss += loss.item()\n",
        "\n",
        "        preds = F.softmax(scores, dim=1).cpu()\n",
        "        targets = labels.cpu()\n",
        "\n",
        "        _val_precision = val_precision(preds, targets)\n",
        "        _val_recall = val_recall(preds, targets)\n",
        "        _val_f1 = val_f1(preds, targets)\n",
        "        _val_accuracy = val_accuracy(preds, targets)\n",
        "\n",
        "        val_progress_bar.set_postfix(f1=_val_f1.item())\n",
        "\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    losses['train'].append(train_loss)\n",
        "\n",
        "    val_loss = val_loss/len(val_loader) \n",
        "    losses['val'].append(val_loss)\n",
        "\n",
        "    precision['train'].append(train_precision.compute().item())\n",
        "    train_precision.reset()\n",
        "    precision['val'].append(val_precision.compute().item())\n",
        "    val_precision.reset()\n",
        "\n",
        "    recall['train'].append(train_recall.compute().item())\n",
        "    train_recall.reset()\n",
        "    recall['val'].append(val_recall.compute().item())\n",
        "    val_recall.reset()\n",
        "\n",
        "    f1_score['train'].append(train_f1.compute().item())\n",
        "    train_f1.reset()\n",
        "    f1_score['val'].append(val_f1.compute().item())\n",
        "    val_f1.reset()\n",
        " \n",
        "    accuracy['train'].append(train_accuracy.compute().item())\n",
        "    train_accuracy.reset()\n",
        "    accuracy['val'].append(val_accuracy.compute().item())\n",
        "    val_accuracy.reset()\n",
        "\n",
        "    print(f'Loss: Train={train_loss:.6f}, Val={val_loss:.6f}')\n",
        "    print(f\"Precision: Train={precision['train'][epoch]:.3f}, Val={precision['val'][epoch]:.3f}\")\n",
        "    print(f\"Recall:    Train={recall['train'][epoch]:.3f},    Val={recall['val'][epoch]:.3f}\")\n",
        "    print(f\"F1_score:  Train={f1_score['train'][epoch]:.3f},  Val={f1_score['val'][epoch]:.3f}\")\n",
        "    print(f\"Accuracy:  Train={accuracy['train'][epoch]:.3f},  Val={accuracy['val'][epoch]:.3f}\")\n",
        "\n",
        "    wandb.log({'Epoch':epoch, 'Train Loss': train_loss, 'Val Loss': val_loss, \n",
        "               'Train Accuracy': accuracy['train'][epoch], 'Val Accuracy': accuracy['val'][epoch],\n",
        "               'Train F1_score': f1_score['train'][epoch], 'Val F1_score': f1_score['val'][epoch]})\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if val_loss <= val_loss_min:\n",
        "      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min, val_loss))\n",
        "      save_checkpoint(epoch, model.state_dict(), optim.state_dict(), losses, precision, recall, f1_score, accuracy, '/content/mvc.pth')\n",
        "      val_loss_min = val_loss\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://wandb.ai/kanish2k/multimodal%20video%20classification/runs/3sow0csf?jupyter=true\" style=\"border:none;width:100%;height:420px\">\n",
              "                </iframe>"
            ],
            "text/plain": [
              "<wandb.jupyter.Run at 0x7f99b2951dd0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "---------- Epoch: 1/10 ----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c84b5c306c14582974adab2bf959a08",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/272 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jovXIR2THLg",
        "outputId": "28508344-827e-4162-876c-b55332d83d65"
      },
      "source": [
        "  # %%wandb\n",
        "  # best_val_f1 = -np.Inf\n",
        "val_loss_min = np.Inf\n",
        "\n",
        "log_interval = 100\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "  \n",
        "  print(f'---------- Epoch: {epoch}/{EPOCHS} ---------- ')\n",
        "  train_loss, manual_train_accuracy = 0.0, 0.0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "    # print('images: ', images.shape, 'labels: ', labels.shape)                   # OUTPUT: imgs: [32, 16, 3, 224, 224] labels: [32]\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    scores = model(images)\n",
        "\n",
        "    loss = criterion(scores, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    preds = F.softmax(scores, dim=1)\n",
        "    print('preds:', preds.shape)                                                # OUTPUT: [32, 8]\n",
        "\n",
        "    print('labels:', labels.shape)                                              # OUTPUT: [32, 8]\n",
        "    # targets = labels.view(-1, 1)\n",
        "    print('targets:', targets.shape)\n",
        "\n",
        "    # calc. metrics\n",
        "    _train_precision = train_precision(preds, labels)\n",
        "    _train_recall = train_precision(preds, labels)\n",
        "    _train_f1 = train_f1(preds, labels)\n",
        "    _train_accuracy = train_accuracy(preds, labels)\n",
        "\n",
        "    _, train_top_class = preds.topk(1, dim=1)\n",
        "    print('train_top_class: ', train_top_class.shape)\n",
        "    print('*train_top_class: ', *train_top_class.shape)\n",
        "\n",
        "    train_equals = train_top_class == labels.view(*train_top_class.shape)\n",
        "  \n",
        "    manual_train_accuracy = torch.mean(train_equals.type(torch.FloatTensor))\n",
        "\n",
        "    stop_time = time.time()\n",
        "\n",
        "    if batch_idx % log_interval == 0 or batch_idx == 0:\n",
        "      print(f'Batch: {batch_idx} || Time: {stop_time - start_time :.2f}s') \n",
        "      print(f'Train Accuracy: {manual_train_accuracy:.6f}')\n",
        "      print(f'Train Accuracy (torchmetrics): {_train_accuracy:.6f}')\n",
        "      break\n",
        "\n",
        "    start_time = stop_time \n",
        "\n",
        "  else:\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "\n",
        "      val_loss, val_accuracy = 0.0, 0.0\n",
        "\n",
        "      for images, labels in val_loader:\n",
        "        \n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        scores = model(images)\n",
        "\n",
        "        loss = criterion(scores, labels)\n",
        "        \n",
        "        val_loss += loss.item()\n",
        "\n",
        "        val_predictions = F.softmax(scores, dim=1) \n",
        "\n",
        "        _val_precision = val_precision(val_predictions, labels)\n",
        "        _train_recall = train_precision(val_predictions, labels)\n",
        "        _val_f1 = val_f1(val_predictions, labels)\n",
        "\n",
        "        _, val_top_class = val_predictions.topk(1, dim=1)\n",
        "      \n",
        "        # calc. correct classes (Compare top_k and labels)\n",
        "        # val_equals = val_top_class == labels.view(*val_top_class.shape)\n",
        "      \n",
        "        # calc. accuracy\n",
        "        # val_accuracy = torch.mean(val_equals.type(torch.FloatTensor))\n",
        "\n",
        "    train_loss = train_loss/len(train_loader)\n",
        "    losses['train_losses'].append(train_loss)\n",
        "\n",
        "    val_loss = val_loss/len(val_loader) \n",
        "    losses['val_losses'].append(val_loss)\n",
        "\n",
        "    precision['train'].append(train_precision.compute().item())\n",
        "    precision['val'].append(val_precision.compute().item())\n",
        "\n",
        "    recall['train'].append(train_recall.compute().item())\n",
        "    recall['val'].append(val_recall.compute().item())\n",
        "\n",
        "    f1valin'].append(train_f1.compute().item())\n",
        "    f1['val'].append(val_f1.compute().item())\n",
        "\n",
        "    print(f'Train Accuracy: {train_accuracy:.6f} || Validation Accuracy: {val_accuracy:.6f}')\n",
        "    print(f'Train Loss: {train_loss:.6f} || Val Loss: {val_loss:.6f}')\n",
        "    wandb.log({'Epoch':epoch, 'Train Loss': train_loss, 'Val Loss': val_loss, 'Train Accuracy': train_accuracy, \n",
        "               'Validation Accuracy': val_accuracy})\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # save model if validation loss has decreased\n",
        "    if val_loss <= val_loss_min:\n",
        "      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min, val_loss))\n",
        "      torch.save(model.state_dict(), '/content/mvc.pth')\n",
        "      val_loss_min = val_loss\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------- Epoch: 1/1 ---------- \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch: 0 || Time: 75.65s\n",
            "Train Accuracy: 0.140625\n",
            "Batch: 10 || Time: 1.85s\n",
            "Train Accuracy: 0.171875\n",
            "Batch: 20 || Time: 68.79s\n",
            "Train Accuracy: 0.250000\n",
            "Batch: 30 || Time: 1.87s\n",
            "Train Accuracy: 0.156250\n",
            "Batch: 40 || Time: 69.68s\n",
            "Train Accuracy: 0.093750\n",
            "Batch: 50 || Time: 1.84s\n",
            "Train Accuracy: 0.281250\n",
            "Batch: 60 || Time: 68.30s\n",
            "Train Accuracy: 0.250000\n",
            "Batch: 70 || Time: 1.87s\n",
            "Train Accuracy: 0.234375\n",
            "Batch: 80 || Time: 65.96s\n",
            "Train Accuracy: 0.281250\n",
            "Batch: 90 || Time: 1.84s\n",
            "Train Accuracy: 0.312500\n",
            "Batch: 100 || Time: 65.95s\n",
            "Train Accuracy: 0.343750\n",
            "Batch: 110 || Time: 1.87s\n",
            "Train Accuracy: 0.421875\n",
            "Batch: 120 || Time: 62.23s\n",
            "Train Accuracy: 0.546875\n",
            "Batch: 130 || Time: 1.84s\n",
            "Train Accuracy: 0.468750\n",
            "Batch: 140 || Time: 58.97s\n",
            "Train Accuracy: 0.343750\n",
            "Batch: 150 || Time: 1.86s\n",
            "Train Accuracy: 0.484375\n",
            "Batch: 160 || Time: 56.84s\n",
            "Train Accuracy: 0.453125\n",
            "Batch: 170 || Time: 1.85s\n",
            "Train Accuracy: 0.437500\n",
            "Batch: 180 || Time: 55.97s\n",
            "Train Accuracy: 0.359375\n",
            "Batch: 190 || Time: 1.88s\n",
            "Train Accuracy: 0.531250\n",
            "Batch: 200 || Time: 53.53s\n",
            "Train Accuracy: 0.578125\n",
            "Batch: 210 || Time: 1.85s\n",
            "Train Accuracy: 0.562500\n",
            "Batch: 220 || Time: 51.03s\n",
            "Train Accuracy: 0.421875\n",
            "Batch: 230 || Time: 1.88s\n",
            "Train Accuracy: 0.390625\n",
            "Batch: 240 || Time: 50.95s\n",
            "Train Accuracy: 0.421875\n",
            "Batch: 250 || Time: 1.90s\n",
            "Train Accuracy: 0.281250\n",
            "Batch: 260 || Time: 50.55s\n",
            "Train Accuracy: 0.703125\n",
            "Batch: 270 || Time: 1.87s\n",
            "Train Accuracy: 0.578125\n",
            "Train Accuracy: 0.562500 || Validation Accuracy: 0.464286\n",
            "Train Loss: 1.589354 || Val Loss: 1.364305\n",
            "Validation loss decreased (inf --> 1.364305).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}